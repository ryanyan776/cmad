diff --git a/cmad/models/effective_stress.py b/cmad/models/effective_stress.py
index 09780d0..5e80ae3 100644
--- a/cmad/models/effective_stress.py
+++ b/cmad/models/effective_stress.py
@@ -41,6 +41,19 @@ def hill_effective_stress(cauchy, params):
     return phi
 
 
+def hybrid_hill_effective_stress(cauchy, params, nn_fun):
+    phi_hill = hill_effective_stress(cauchy, params)
+    hydro_cauchy = jnp.trace(cauchy) / 3.
+    s = cauchy - hydro_cauchy * jnp.eye(3)
+    flat_s = jnp.array([s[0, 0], s[1, 1], s[2, 2],
+        s[0, 1], s[0, 2], s[1, 2]])
+    phi_discrepancy = nn_fun(flat_s,
+        params["effective stress"]["neural network"])
+
+    return phi_hill + phi_discrepancy[0]
+    #return phi_discrepancy[0] # NN only fit
+
+
 # only working for diagonal cauchy stress now
 def hosford_effective_stress(cauchy, params):
     vm_stress = J2_effective_stress(cauchy, params)
diff --git a/cmad/neural_networks/input_convex_neural_network.py b/cmad/neural_networks/input_convex_neural_network.py
new file mode 100644
index 0000000..2a53b1c
--- /dev/null
+++ b/cmad/neural_networks/input_convex_neural_network.py
@@ -0,0 +1,74 @@
+"""
+Adapted from the jax tutorial:
+https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html
+"""
+
+import numpy as np
+import matplotlib.pyplot as plt
+
+import jax.numpy as jnp
+
+from functools import partial
+
+from jax import jit, grad, tree_map
+from jax.nn import relu, sigmoid, softplus
+from jax.flatten_util import ravel_pytree
+from jax.tree_util import tree_unflatten, tree_flatten
+
+
+def forward_with_offset(x, params, input_scaler, output_scaler):
+    xs = input_scaler.scale_ * x + input_scaler.min_
+    scaled_output = forward(xs, params) - forward(jnp.zeros_like(xs), params)
+    output = (scaled_output - output_scaler.min_) / output_scaler.scale_
+    return output
+
+
+def forward(x, params):
+    activation = softplus
+    *x_hidden, x_last = params["x params"]
+    *z_hidden, z_last = params["z params"]
+
+    z = activation(x @ x_hidden[0]["weights"] + x_hidden[0]["biases"])
+    for x_layer, z_layer in zip(x_hidden[1:], z_hidden):
+        z = activation(z @ z_layer["weights"] + x @ x_layer["weights"] \
+          + x_layer["biases"])
+
+    return z @ z_last["weights"] + x @ x_last["weights"] \
+           + x_last["biases"]
+
+
+class InputConvexNeuralNetwork():
+
+    def __init__(self, layer_widths: list,
+                 input_scaler, output_scaler,
+                 seed: int = 22):
+        self._init_params(layer_widths, seed)
+        self.evaluate = partial(forward_with_offset,
+                                input_scaler=input_scaler,
+                                output_scaler=output_scaler)
+
+    def _init_params(self, layer_widths: list, seed: int):
+        np.random.seed(seed)
+        num_x_trainable_layers = len(layer_widths) - 1
+        num_z_trainable_layers = len(layer_widths) - 2
+        x_layers_idx = np.arange(num_x_trainable_layers, dtype=int)
+        z_layers_idx = np.arange(num_z_trainable_layers, dtype=int)
+        x_params = [None] * num_x_trainable_layers
+        x_input_widths = [layer_widths[0]] * num_x_trainable_layers
+        z_params = [None] * num_z_trainable_layers
+        x_layer_props = zip(x_layers_idx, x_input_widths, layer_widths[1:])
+        z_layer_props = zip(z_layers_idx, layer_widths[1:-1], layer_widths[2:])
+
+        for idx, num_in, num_out in x_layer_props:
+            x_params[idx] = dict(weights=np.random.normal(
+                size=(num_in, num_out))
+                * np.sqrt(2. / num_in), biases=np.ones(num_out))
+
+        # abs initialization for monotonic networks
+        for idx, num_in, num_out in z_layer_props:
+            z_params[idx] = dict(weights=np.abs(np.random.normal(
+                size=(num_in, num_out))
+                * np.sqrt(2. / num_in)))
+
+        self.x_params = x_params
+        self.z_params = z_params
